{"cells":[{"cell_type":"markdown","source":["Creating tables in a Lakehouse (such as in Microsoft Fabric) involves a few steps, depending on whether you’re creating them from scratch, from files, or from existing datasets."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5cce7d8e-b618-450e-a146-bebddcd6b4de"},{"cell_type":"markdown","source":["# Step 1 – Open or Create a Lakehouse\n","\n","1. Go to Microsoft Fabric Portal → Sign in with your Microsoft 365 account.\n","2. On the left menu, click Workspaces → Open your desired workspace.\n","3. Click New → Select Lakehouse (you’ll find it under Data Engineering).\n","\n","Give it a name → Click Create.\n","\n","- Once it opens, you’ll see two main panels:\n","\n","- Tables – for structured tables stored in Delta format."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4372cd50-8210-4dd2-8837-86b278a80fd0"},{"cell_type":"markdown","source":["# Step 2 – Understand the Table Types\n","\n","In a Lakehouse, you can have:\n","\n","Managed Tables\n","\n","- Stored in the Tables section.\n","\n","- Backed by Delta Lake (ACID transactions).\n","\n","- Can be queried directly via SQL endpoint.\n","\n","External Tables\n","\n","- Reference data in the Files folder (or external OneLake/ADLS location).\n","\n","- Useful for large raw datasets without moving them."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a175bb3-947b-4a24-a8ea-3fce79e17339"},{"cell_type":"markdown","source":["# Step 3 – Ways to Create a Table\n","\n","### A. Create from the UI (File or Blank Table)\n","\n","In the Lakehouse, click New Table → Choose From file or Blank Table.\n","\n","From File:\n","\n","- Select a file from:\n","\n","    - Local computer\n","     \n","    - OneLake\n","     \n","    - Other cloud storage (via shortcut)\n","\n","- Supported formats: CSV, TSV, Parquet, JSON, Delta.\n","\n","- Choose:\n","\n","    - Table name\n","    \n","    - Delimiter (for CSV/TSV)\n","    \n","    - Whether the first row is headers\n","\n","    - Schema mapping (data types)\n","\n","- Click Create.\n","\n","Blank Table:\n","\n","- Define table name.\n","\n","- Add columns:\n","\n","    - Name\n","     \n","    - Data type (STRING, INT, DATE, etc.)\n","    \n","    - Nullable or not\n","\n","- Click Create.\n","\n","### B. Create via SQL Endpoint\n","\n","- In the Lakehouse, click SQL Endpoint (top right).\n","- \n","- Run SQL DDL commands:\n","\n","CREATE TABLE SalesData (\n","    SaleID INT,\n","    ProductName STRING,\n","    Quantity INT,\n","    SaleDate DATE\n",")\n","USING DELTA;\n","\n","\n","- USING DELTA ensures ACID and versioning support.\n","\n","- This creates a managed table in the Tables folder.\n","\n","### C. Create via Notebook (PySpark / SparkSQL)\n","\n","1. Click New Notebook in the Lakehouse.\n","\n","2. Run:\n","\n","###### Sample data\n","data = [(1, \"Laptop\", 5, \"2025-08-15\"),\n","        (2, \"Mouse\", 10, \"2025-08-16\")]\n","columns = [\"SaleID\", \"ProductName\", \"Quantity\", \"SaleDate\"]\n","\n","###### Create Spark DataFrame\n","df = spark.createDataFrame(data, columns)\n","\n","###### Save as table\n","df.write.format(\"delta\").saveAsTable(\"SalesData\")\n","\n","### Or using Spark SQL:\n","\n","CREATE TABLE SalesData\n","\n","USING DELTA\n","\n","AS SELECT 1 AS SaleID, 'Laptop' AS ProductName, 5 AS Quantity, DATE('2025-08-15') AS SaleDate;\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"365d855b-323a-4909-9b0b-63646d88bb09"},{"cell_type":"markdown","source":["# Step 4 – Verify the Table\n","\n","- In the Tables pane → Your new table will appear.\n","\n","- Click it → View Schema and Sample Data.\n","\n","- You can query it in:\n","\n","    - SQL Endpoint\n","\n","    - Notebooks\n","\n","    -  Power BI (direct Lakehouse connection)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c768425-dcf9-4b3a-ae0e-15c85e2c6212"},{"cell_type":"markdown","source":["# Step 5 – Best Practices\n","\n","- Use Delta format for all analytical workloads.\n","\n","- Keep schema consistent — mismatches will cause query failures.\n","\n","- For ingestion automation, use:\n","\n","    - Dataflows Gen2\n","\n","    - Pipelines\n","\n","    - Notebook scripts\n","\n","- Avoid storing raw files directly in Tables folder; use the Files area for staging."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78f38f49-aa2e-48d7-bbac-1c3ed27c19b9"},{"cell_type":"markdown","source":["# Step 6 – Example Full Workflow\n","\n","1. Create Lakehouse → Upload a CSV (sales.csv) into Files.\n","\n","2. Open Notebook → Load CSV as DataFrame:\n","\n","df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Files/sales.csv\")\n","\n","3. Write it as a managed table:\n","\n","df.write.format(\"delta\").saveAsTable(\"SalesData\")\n","\n","4. Switch to SQL Endpoint:\n","\n","SELECT * FROM SalesData WHERE Quantity > 5;\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77cda400-83ea-4001-aa31-73c44722d32e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}
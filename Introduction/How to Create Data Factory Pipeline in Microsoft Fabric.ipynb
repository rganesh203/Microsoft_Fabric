{"cells":[{"cell_type":"markdown","source":["Step-by-step, detailed guide on how to create a Data Factory pipeline in Microsoft Fabric — including all the prerequisites, components, and configurations. I'll also include an example use case to make it clear."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"228275ba-7fd9-4811-9886-3c76c554bff6"},{"cell_type":"markdown","source":["# 1) Prerequisites & setup\n","\n","1. Fabric workspace & role\n","\n","You need a Fabric workspace (trial or capacity) and permissions to create/edit items (Admin, Member, or Contributor typically). See Fabric workspace roles/permissions. \n","\n","2. Connections (replace “linked services”)\n","\n","In Fabric, connections are managed centrally under Settings → Manage connections and gateways. You’ll create reusable connections here (SQL, ADLS Gen2, Blob, Salesforce, etc.).\n","\n","3. Credentials & secret storage\n","\n","Connections support auth types like OAuth2 and Service Principal. Fabric also supports Azure Key Vault–stored secrets for connection authentication (preview).\n","\n","4. On-premises / private network sources\n","\n","If your source is on-prem or in a private network, set up On-premises data gateway or Virtual network data gateway."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fefe3fb-bc3c-43e5-9ede-098420435dad"},{"cell_type":"markdown","source":["# 2) Create your first pipeline\n","\n","1. In the Fabric portal, switch to the Data Factory experience.\n","\n","2. New → Data pipeline. Give it a descriptive name (e.g., pl_ingest_sales_daily).\n","\n","3. You’ll land on the pipeline canvas (Activities panel on the left; Properties pane below).\n","\n","Microsoft’s “Create your first pipeline” quickstart shows the exact UI and where things live"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"632b3e9f-cc2f-41ab-88c4-47015865e207"},{"cell_type":"markdown","source":["# 3) Add a Copy activity (fastest: Copy Assistant)\n","\n","Most pipelines start by moving data. The Copy Assistant wizard builds a Copy activity for you:\n","\n","1. On the canvas, select Copy data → Use copy assistant. \n","\n","2. Source\n","\n","    Pick your source type, then choose an existing connection or Create new connection. Select the objects (tables/files). \n","\n","3. Destination\n","\n","    Choose your destination (e.g., Lakehouse, Data Warehouse, ADLS, Blob, etc.). Map to a table or file/folder path. \n","\n","\n","4. Review → Finish to drop the activity on the canvas. You can fine-tune in the activity tabs (Source, Destination, Mapping, Settings). \n","\n","Key Copy settings to know (Settings tab):\n","\n","    Throughput (aka intelligent throughput / DIUs), parallelism, fault tolerance, staging, logging, compression. Fabric documents each knob and the JSON equivalents. \n","\n","\n","Gateway rule: if you copy between two on-prem data stores, both ends must use the same gateway (otherwise stage in the cloud in two hops)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"125458b9-aea9-417a-a052-a9faf773eb54"},{"cell_type":"markdown","source":["# 4) Transform data (optional)\n","\n","After (or before) Copy, add transformation activities:\n","\n","    Notebook (Spark on Fabric) for Python/Scala data prep. \n","\n","\n","    Dataflow Gen2 for low-code transformations. (Listed under Data transformation activities.) \n","\n","    SQL Script / Stored Procedure to push work down to SQL engines. \n","\n","\n","Add these from Activities → Data transformation; configure in the properties pane. See Fabric’s Activity overview for what’s available (including HDInsight, Spark Job Definition)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4446f18d-6f9b-488b-bd98-85960fd2b6d7"},{"cell_type":"markdown","source":["# 5) Orchestrate with control flow\n","\n","Fabric has a rich set of control activities (very similar to ADF):\n","\n","    - If Condition, Switch, ForEach, Until, Wait, Set/Append Variable, Get Metadata, Lookup, Web/Webhook, Invoke pipeline, etc. \n","\n","Typical orchestration pattern:\n","\n","    - Lookup list of tables/files → ForEach over the list → inside loop Copy (and branch If on row counts), finally Invoke pipeline to modularize big workflows."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9042e33d-7f26-4702-a695-408dd356177f"},{"cell_type":"markdown","source":["# 6) Parameterization & dynamic content (must-have)\n","\n","Define pipeline parameters (top of the canvas) and use them across activities (e.g., file paths, table names).\n","\n","In activity fields, choose Add dynamic content to reference @pipeline().parameters.MyParam, variables, or system values.\n","\n","For Copy to Workspace items (Lakehouse/Data Warehouse/KQL DB), Fabric shows how to pass the object ID via parameters (handy for promoting between workspaces/environments). \n","Microsoft Learn\n","\n","#### Pro tip – common expressions:\n","\n","Build a dated folder: \n","    - @concat('/landing/sales/date=', formatDateTime(utcNow(),'yyyy-MM-dd'), '/')\n","\n","Use Lookup result: \n","    - @activity('GetTables').output.value\n","\n","Use ForEach item: \n","    - @item()"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"694bf4b1-27f1-4fbb-a3b9-72c7126894df"},{"cell_type":"markdown","source":["# 7) Scheduling vs. event-based triggers\n","### A) Wall-clock schedules (every N minutes/hours/days/weeks)\n","\n","In the pipeline editor, select Home → Schedule and define frequency, start/end dates, and time zone. (You can add multiple schedules per pipeline in the latest experience; schedule definitions live with the pipeline.) \n","\n","The Pipeline runs concept article shows the Schedule button and on-demand runs. \n","\n","Note: Historically Fabric required an end date. If you want “run forever,” set an end date far in the future. Check your tenant’s current experience and docs for exact behavior. \n","\n","### B) Event triggers (file-driven, job events, workspace events)\n","\n","Fabric now supports storage/event triggers wired through Real-Time Intelligence (formerly Data Activator) and eventstreams.\n","\n","In your pipeline, select Home → Trigger to open Set alert and pick OneLake or Azure Blob events, filter by folder/file patterns, then create the trigger (a Reflex item is created in your workspace). \n","\n","Using event data inside your pipeline:\n","Built-in trigger parameters provide file/folder from the event. Example:\n","\n","@pipeline()?.TriggerEvent?.FileName\n","Use the Trigger parameters tab in Expression Builder to insert them. (The ?. handles nulls when you manually run without an event.)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad4f7df5-0128-445a-99fe-3aa7be6a65e1"},{"cell_type":"markdown","source":["# 8) Run & monitor\n","\n","Manual run: Home → Run (Fabric prompts to Save and run). Watch the Output pane for activity progress. \n","\n","### Monitor:\n","\n","Open the pipeline and select Run history (or go to Pipeline runs in Data Factory docs). You’ll see Succeeded/Failed, duration, rows read/written, throughput, etc. Drill into Activity run details for errors and metrics."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35a917df-edbd-490a-b7fb-c12dfe733dd3"},{"cell_type":"markdown","source":["# 9) Worked example (copy Azure SQL → Lakehouse, then transform)\n","\n","1. Connections\n","\n","Create two connections: Azure SQL DB (Service Principal/OAuth2) and Lakehouse (Workspace) under Manage connections and gateways. Test both.\n","\n","2. Pipeline\n","\n","New → Data pipeline → Copy Assistant.\n","\n","Source: Azure SQL Database → select Sales.SalesOrderHeader & Sales.SalesOrderDetail. \n","\n","Destination: Lakehouse → Tables → create tables sales_order_header and sales_order_detail. Review mappings. \n","\n","Settings:\n","\n","Start with Intelligent throughput = Auto and Parallelism left default; enable Fault tolerance if you expect bad rows. Consider Staging only for tricky cross-region or large loads. \n","\n","3. Transform (optional)\n","\n","Add a Notebook activity after Copy to clean/enrich data (e.g., dedupe, cast types), writing to a curated Lakehouse table. \n","\n","4. Orchestrate\n","\n","Wrap the two Copy activities in a ForEach driven by a parameterized table list, or use Invoke pipeline to keep ingestion modular. \n","\n","5. Parameterize\n","\n","Add pipeline parameters like Environment, LandingPath, LakehouseId. Use Add dynamic content in Copy Source/Destination to compose paths and pass the Lakehouse object ID. \n","\n","6. Schedule or event\n","\n","Schedule daily at 06:00 with your time zone. Or create an event trigger for file arrival to kick off the pipeline. \n","\n","7. Run & monitor\n","\n","Save and run, check Output; open Run details to confirm rows read/written and throughput.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc5b95b5-5434-4ffa-8c07-bace0e2ba694"},{"cell_type":"markdown","source":["# 10) Performance & reliability tips\n","\n","- Throughput & parallelism: Start with Auto throughput; raise only if bottlenecked and the source/destination can handle it. \n","\n","- Partitioning & pushdown: For large SQL sources, use Query/Stored procedure and partition predicates; for files, use folder partitioning. \n","\n","- Fault tolerance & logging: Enable skip incompatible rows and logging for messy files. \n","\n","- Gateway constraints: One copy activity can only use one on-prem gateway—stage if you need to bridge two different gateways. \n","\n","- Security: Prefer Service principal/managed identities and Key Vault-stored secrets (preview) over user creds.\n","\n","- Item limits: By default up to 120 activities per pipeline (includes inner activities). If you’re approaching that, break into child pipelines and Invoke pipeline."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d1d857a-7750-49be-9178-a2750172e5de"},{"cell_type":"markdown","source":["# 11) Where to find “what is possible” at a glance\n","\n","- Activity overview (all movement, transform, control flow activities + general settings like timeout/retry/secure input or output). \n","\n","- Copy activity (assistant, mappings, settings, parameters). \n","\n","- Schedule pipelines and Pipeline runs (on-demand vs. scheduled). \n","\n","- Event triggers (OneLake/Azure Blob, Reflex integration, trigger parameters). \n"," \n","- Monitoring runs (UI walkthrough). \n","\n","- Connections & gateways (central management)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99b5a427-905e-47b8-94af-e59944fa3402"},{"cell_type":"markdown","source":["Create a full, detailed blueprint for creating a Data Factory pipeline in Microsoft Fabric with a real-world example — including step-by-step UI walkthrough, parameters, triggers, and a visual workflow diagram.\n","\n","Let's assume this example scenario for clarity:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e220e918-e610-41e3-b104-9277eb79493d"},{"cell_type":"markdown","source":["Use Case:\n","Ingest Sales data (CSV) from Azure Data Lake Storage Gen2 into a Microsoft Fabric Lakehouse every day at 2:00 AM.\n","\n","After copying, run a PySpark notebook to transform data and create an aggregated table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdb5b927-d8ec-4df8-8b19-b8d2cbef6cb6"},{"cell_type":"markdown","source":["# 12. Prerequisites\n","\n","Before building the pipeline:\n","\n","✅ Microsoft Fabric Workspace → Create or select an existing workspace.\n","\n","✅ Enable Data Factory → Go to Admin Portal → Capacity Settings → Enable Data Factory.\n","\n","✅ Prepare Source & Destination:\n","\n","    - Source: CSV files in Azure Data Lake Gen2.\n","\n","    - Destination: Fabric Lakehouse.\n","\n","✅ Set Up Connections:\n","\n","    - Go to Settings → Manage connections.\n","\n","Create a connection for:\n","\n","    - Azure Data Lake Gen2 → Use OAuth2 or Service Principal.\n","\n","    - Lakehouse → Select your workspace lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3b6487b-c9ba-42bd-b094-a3ba8de2c273"},{"cell_type":"markdown","source":["# 13. Create a New Data Pipeline\n","\n","Go to Microsoft Fabric → Data Factory experience.\n","\n","- Click + New → Data pipeline.\n"," \n","- Name it: pl_ingest_sales_data.\n","\n","- You’ll land on the pipeline canvas."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16c83b32-bb6b-4cc9-8bf1-73cae9fbda44"},{"cell_type":"markdown","source":["# 14. Add Activities to the Pipeline\n","### Step 14.1: Copy Data Activity\n","\n","We’ll use the Copy Assistant for simplicity.\n","\n","1. Click + → Copy data → Use Copy Assistant.\n","\n","2. Select Source:\n","\n","    - Type: Azure Data Lake Gen2.\n","\n","    - Select your connection.\n","\n","    - Choose the container & folder where CSV files are stored.\n","\n","3. Select Destination:\n","\n","    - Type: Fabric Lakehouse.\n","\n","    - Choose your Lakehouse connection.\n","\n","    - Select Tables and name it Sales_Raw.\n","\n","4. Mapping:\n","\n","    - Auto-map columns (you can manually map if needed).\n","\n","5. Settings:\n","\n","    - Enable Skip incompatible rows.\n","\n","    - Enable Fault tolerance logging to capture rejected records.\n","\n","6. Click Finish — your Copy Data activity will appear on the canvas.\n","\n","### Step 14.2: Add a Notebook Activity (Transformation)\n","\n","We’ll run a PySpark notebook to clean & aggregate data.\n","\n","1. Drag Notebook activity from Activities → Data Transformation.\n","\n","2. Configure:\n","\n","    - Notebook path → Select your Fabric notebook (e.g., Sales_Aggregation.ipynb).\n","    \n","    - Parameters → Pass the Lakehouse table name as a parameter.\n","\n","3. Connect the Copy Data activity → Notebook activity using a success dependency.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e0ba41ea-85a3-4615-b6cc-756941a431b8"},{"cell_type":"markdown","source":["# 15. Parameterize the Pipeline\n","\n","Make your pipeline dynamic:\n","\n","1. Go to Pipeline → Parameters.\n","\n","2. Add:\n","\n","    - p_input_path → /sales/raw/\n","    \n","    - p_output_table → Sales_Raw\n","\n","3. In Copy Data Source → File path:\n","    Use:\n","        @pipeline().parameters.p_input_path\n","\n","4. In Notebook Activity:0\n","    Pass: \n","        @pipeline().parameters.p_output_table\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75ee18b1-7723-4f44-92d7-55df6c1d402e"},{"cell_type":"markdown","source":["# 16. Add a Trigger\n","### Option 1: Scheduled Trigger (Daily)\n","\n","1. Go to Pipeline → Add Trigger → New Trigger.\n","\n","2. Name: Daily_Sales_Trigger.\n","\n","3. Type: Schedule.\n","\n","4. Start Time: 02:00 AM.\n","\n","5. Recurrence: Every 1 Day.\n","\n","### Option 2: Event Trigger (File Arrival)\n","\n","- Trigger pipeline when a new CSV arrives in Azure Data Lake:\n","\n","    - Choose Event-based trigger.\n","\n","    - Select your Azure Data Lake connection.\n","\n","    - Set folder path to /sales/raw/.\n","\n","    - The pipeline runs automatically when a new file is dropped."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79295d78-d7be-48e7-a2d7-5dc96a74c087"},{"cell_type":"markdown","source":["# 17. Validate and Publish\n","\n","1. Click Validate → Fix any warnings or errors.\n","\n","2. Click Publish all to make your pipeline live."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df6d4274-8f78-418a-ae53-1bd57a83dcc5"},{"cell_type":"markdown","source":["# 18. Monitor the Pipeline\n","\n","1. Go to the Monitor tab in Data Factory.\n","\n","2. You can see:\n","\n","- Success / Failed runs.\n","\n","- Execution duration.\n","\n","- Rows read / written.\n","\n","3. Drill into activity logs for troubleshooting."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8138ea7d-d023-4e7d-8b00-078ff26297b6"},{"cell_type":"markdown","source":["# 19. Visual Workflow Diagram\n","\n","Here's the high-level flow:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b801a817-6c31-4879-9307-df7f8074206c"},{"cell_type":"markdown","source":["          ┌────────────────────┐\n","          │   Azure Data Lake  │\n","          │   (CSV Source)     │\n","          └────────┬───────────┘\n","                   │\n","           Copy Data Activity\n","                   │\n","                   ▼\n","          ┌────────────────────┐\n","          │   Fabric Lakehouse │\n","          │   (Raw Table)      │\n","          └────────┬───────────┘\n","                   │\n","          Notebook Activity\n","    (Data Cleaning & Aggregation)\n","                   │\n","                   ▼\n","          ┌────────────────────┐\n","          │ Lakehouse (Curated)│\n","          │ Sales_Aggregated   │\n","          └────────────────────┘\n","\n","   Trigger → Daily at 2:00 AM / Event-driven\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0aef1d2f-afbc-4935-b6ed-66c59c159e19"},{"cell_type":"markdown","source":["# 20. Best Practices\n","\n","Use parameters for folder paths, table names, and environment configs.\n","\n","Store secrets in Azure Key Vault instead of hardcoding credentials.\n","\n","Enable retry policies for all critical activities.\n","\n","Use parallelism for large datasets (Copy settings → Degree of Copy).\n","\n","Always monitor pipelines after publishing to catch issues early."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a340c1d-b86b-4403-856c-6d080e6abfa2"},{"cell_type":"markdown","source":["# Summary Table\n","| **Step** | **Action**                     | **Outcome**                  |\n","| -------- | ------------------------------ | ---------------------------- |\n","| **1**    | Set up connections             | Ready to access data         |\n","| **2**    | Create pipeline                | Workspace pipeline created   |\n","| **3**    | Add Copy + Notebook activities | Ingest + transform data      |\n","| **4**    | Add parameters                 | Pipeline becomes reusable    |\n","| **5**    | Add trigger                    | Automates ingestion          |\n","| **6**    | Validate & publish             | Deploys to Fabric            |\n","| **7**    | Monitor                        | Check success & troubleshoot |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37451b3e-5644-48a3-8033-bb9904db2c33"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","source":["Here’s a step-by-step guide to creating a Lakehouse in Microsoft Fabric — including what it is, how it works, and the actual clicks you’ll need in the Fabric UI."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd463446-e88b-4cbd-880e-6c0e9320831f"},{"cell_type":"markdown","source":["# 1. Understanding the Lakehouse in Microsoft Fabric\n","In Microsoft Fabric, a Lakehouse is a data architecture pattern that merges:\n","\n","- Data Lake → Stores raw files in their native format.\n","\n","- Data Warehouse → Adds relational table structure, indexing, and SQL query support.\n","\n","The Fabric Lakehouse is backed by OneLake, Microsoft’s unified storage layer.\n","You get two layers automatically:\n","\n","- /Files → stores your unstructured/semi-structured data.\n","\n","- /Tables → stores structured tables in Delta format for transactional & analytic queries.\n","\n","Why it’s powerful:\n","\n","- Zero-ETL between storage and analytics.\n","\n","- Same data can be used in Spark notebooks, SQL queries, and Power BI without duplication.\n","\n","- Supports both schema-on-read (data lake style) and schema-on-write (warehouse style)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19ebb352-87d0-4249-9d76-4004db2c9316"},{"cell_type":"markdown","source":["# 2. Prerequisites Before You Begin\n","| Requirement                                     | Why You Need It                         |\n","| ----------------------------------------------- | --------------------------------------- |\n","| **Microsoft Fabric license (Trial or Premium)** | Enables Lakehouse creation              |\n","| **Workspace in Fabric**                         | Your Lakehouse lives inside a workspace |\n","| **Contributor role or higher**                  | Needed to create/edit a Lakehouse       |\n","| **Data source access**                          | For ingestion from external systems     |\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eaa83243-a1dc-4e70-92f8-3b52eaa5f551"},{"cell_type":"markdown","source":["# 3. Step-by-Step Guide to Creating a Lakehouse\n","\n","### Step 1 – Open Microsoft Fabric\n","\n","- Go to https://app.fabric.microsoft.com.\n","\n","- Sign in with your work or school account.\n","\n","### Step 2 – Choose a Workspace\n","\n","- In the left navigation, click Workspaces.\n","\n","- Select an existing workspace.\n","\n","- If you don’t have one:\n","\n","    1. Click + New workspace.\n","\n","    2. Give it a name (e.g., Data_Engineering).\n","\n","    3. Assign Fabric capacity.\n","\n","    4. Click Save.\n","\n","### Step 3 – Create the Lakehouse\n","1. In the workspace, click + New → More options → Data engineering → Lakehouse.\n","(Or directly from the home page: click Create → Lakehouse.)\n","\n","2. Enter:\n","\n","    - Name: e.g., Customer360_Lakehouse\n","    - Description: e.g., “Stores raw and curated customer data.”\n","\n","3. Click Create.\n","\n","4. You’ll land on the Lakehouse UI with two main folders:\n","\n","    - Tables\n","    - Files\n","\n","### Step 4 – Load Data into the Lakehouse\n","Option 1: Upload Files Manually\n","1. Click New data → Upload.\n","\n","2. Drag & drop or browse for CSV, JSON, Parquet, Excel, or Delta files.\n","\n","3. Files will be stored in the /Files directory.\n","\n","Option 2: Ingest from a Data Source\n","1. Click Get Data → choose a connector:\n","\n","    Azure Data Lake Storage Gen2\n","\n","    SQL Database\n","\n","    Blob Storage\n","\n","    Amazon S3\n","\n","    APIs, SaaS apps (Salesforce, Dynamics 365, etc.)\n","\n","2. Configure connection credentials.\n","\n","3. Map to Tables or Files.\n","\n","4. Run the ingestion.\n","\n","Option 3: Use Data Pipelines\n","    Create a Data Pipeline in Fabric and add a Copy Data activity targeting the Lakehouse.\n","\n","### Step 5 – Organize Data\n","    Use /Files/raw for unprocessed data.\n","\n","    Use /Files/processed or /Tables for clean, transformed data.\n","\n","    Always store curated datasets in Delta format for best performance.\n","\n","### Step 6 – Query the Data\n","\n","Using SQL Endpoint\n","\n","    In the Lakehouse, click SQL endpoint (top-right).\n","\n","    Run queries like:\n","\n","SELECT * FROM SalesData\n","WHERE Region = 'APAC';\n","\n","    Great for BI teams who know SQL.\n","\n","Using Spark Notebooks\n","    Click New Notebook in the Lakehouse.\n","\n","    Choose PySpark, Scala, or Spark SQL.\n","\n","    Example PySpark:\n","\n","df = spark.read.format(\"delta\").load(\"Tables/SalesData\")\n","df.filter(df.Region == \"APAC\").display()\n","\n","### Step 7 – Connect to Power BI\n","\n","    In the Lakehouse → click New report → Power BI.\n","\n","    Power BI automatically detects the SQL endpoint.\n","\n","    Create visuals and publish dashboards.\n","\n","\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7d9492d5-5b67-4bc6-9b21-31472b69a8da"},{"cell_type":"markdown","source":["# 4. Behind the Scenes: Lakehouse Architecture\n","\n","[Data Sources]\n","\n","    │\n","    ├── Ingestion (Pipelines, Dataflows, Notebooks)\n","    │\n","\n","[Fabric Lakehouse in OneLake]\n","\n","    ├── /Files (Raw Data)\n","    ├── /Tables (Delta Tables)\n","    │\n","\n","    ├── Spark Engine  ⇆  SQL Endpoint\n","    │\n","\n","[Consumption]\n","\n","    ├── Power BI\n","    ├── Notebooks (ML/AI)\n","    ├── External SQL Clients\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0721942d-ad89-4efd-acb3-b010e8acba8d"},{"cell_type":"markdown","source":["# 5. Best Practices\n","- Use Delta format for large datasets → supports ACID transactions & time travel.\n","\n","- Maintain data zones: Raw → Curated → Aggregated.\n","\n","- Use meaningful table names (avoid spaces).\n","\n","- Schedule ingestion using Pipelines to keep data fresh.\n","\n","- Enable Shortcuts in OneLake to reuse data across workspaces.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61bd8731-8e96-4a05-9076-72b7a6e63ebb"},{"cell_type":"markdown","source":["If you want, I can create a full visual tutorial with screenshots so it looks exactly like Microsoft Fabric’s UI while walking through the Lakehouse creation process. That would make it training-ready for a team."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c890adee-bbc1-48c6-a2a0-0115dc1e763c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}